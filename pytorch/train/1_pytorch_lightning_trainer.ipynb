{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "142c5c65-b0ca-4a5c-85dd-34e663abb44a",
   "metadata": {},
   "source": [
    "- datamodule은 lightning transformers를 사용하자.\n",
    "- 모델은 그냥 pytorch lightning의 예제를 활용하자. (https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/text-transformers.html)\n",
    "    - 이게 좀 더 직관적이고 pytorch lightning을 이해하기 좋음.\n",
    "- 기본 pytorch랑 같이 살펴보자.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dbc9ad-f369-4fe5-a6ba-fbfe79a21043",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb0f05e-ca55-4e60-8e93-a0457f7176a6",
   "metadata": {},
   "source": [
    "## 1. From `nn.Module` to `pl.LightningModule`\n",
    "\n",
    "### pytorch의 기본 모델링\n",
    "- NN은 comprise of layers/modules that perform operations on data임.\n",
    "    - torhc.nn은 NN을 조립하기 위한 building block을 제공함.\n",
    "- 모든 pytorch의 모듈은 nn.Module을 subclassing하여 정의됨.\n",
    "    - init에는 building block으로 사용할 layer들을 initialize함.\n",
    "    - forward에는 operation을 정의함.\n",
    "- forward에 정의한 값들은 그대로 callable하게 사용할 수 있음.\n",
    "\n",
    "```python\n",
    "from torch import nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "        \n",
    "```\n",
    "\n",
    "### pl.LightningModule\n",
    "- pl.LightningModule을 상속하되 `__init__`과 `forward`를 그대로 갖고 오자.\n",
    "\n",
    "```python\n",
    "\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class NeuralNetwork(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "        \n",
    "```\n",
    "- `pl.LightningModule`은 위의 `nn.Module`의 기본 구조에 학습과 관련된 추가적인 기능들을 메서드로 추가했다.\n",
    "    - training_step\n",
    "    - validation_step\n",
    "    - test_step\n",
    "    - configure_optimizers\n",
    "    - and more...    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee3abec-a09a-4a4a-8903-3b05744b0841",
   "metadata": {},
   "source": [
    "---\n",
    "pl.LightningModule의 주요 메서드들은 다양한 리턴 형태를 갖는다. 이를 이해하는 것이 pytorch lightning 코드를 이해하는데 도움이 될것이다.\n",
    "\n",
    "### ```configure_optimizers```\n",
    "- configure_optimizers 메서드에 대한 리턴은 총 **6가지 방식**이 가능함.\n",
    "    - https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#configure-optimizers\n",
    "- 여기서는 Two List 방식의 return을 사용함.\n",
    "    - 첫 번째 list는 optimizer에 대한 configure를 갖고 있는 dictionary임. \n",
    "    - 두 번째 list는 scheduler에 대한 configure를 갖고 있는 dictioanry다.\n",
    "    \n",
    "### ```training_step```\n",
    "- loss tensor 리턴을 하거나 loss를 key 값으로 하는 dictioanry를 리턴함.\n",
    "- https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#training-step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67487e1e-806c-4845-bd42-2827be4a8e31",
   "metadata": {},
   "source": [
    "## 2. Feature-based Approach with Pretrained Language Model\n",
    "- HuggingFace AutoTokenizer + AutoModl + AutoConfig 기반의 Pytorch Lightgning 모델 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f3d525-877d-4ce0-857f-44e8aacb2688",
   "metadata": {},
   "source": [
    "### define config arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d12a82b-dfbc-40da-b3bf-9fa05779e323",
   "metadata": {},
   "source": [
    "**data_args**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2474555c-09f6-432b-a908-f4590b29791d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from lightning_transformers.core.nlp import HFTransformerDataConfig\n",
    "\n",
    "# load data_args configure\n",
    "args = OmegaConf.load('dm_config/ynat_base.yaml')\n",
    "data_args = HFTransformerDataConfig(batch_size=args.batch_size)\n",
    "data_args = OmegaConf.create(vars(data_args))\n",
    "data_args = OmegaConf.create(data_args)\n",
    "data_args = OmegaConf.merge(data_args, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654dbae3-63b8-4f1a-b139-c670cf82e977",
   "metadata": {},
   "source": [
    "**model_args and training_args**\n",
    "- huggingface Trainer's TrainingArguments and ModelArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04ac1f76-8d06-4603-8d29-f20b0d370fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from dataclasses import dataclass, field\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n",
    "            \"with private models).\"\n",
    "        },\n",
    "    )\n",
    "               \n",
    "model_args = ModelArguments(model_name_or_path='klue/roberta-small')\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='ckpt/ynat',\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    do_predict=False,\n",
    "    evaluation_strategy='steps',\n",
    "    logging_strategy='steps',\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    seed=42,\n",
    "    metric_for_best_model='macro-f1',\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177fa3a0-66d1-43a1-bea4-c62a805209cb",
   "metadata": {},
   "source": [
    "**Load DataModules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dcbe009-5e8c-45e1-bb34-da3cae98db43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from src.datamodules.task.nlp import TextClassificationDataModule\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)\n",
    "dm = TextClassificationDataModule(tokenizer, data_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26c05df-1173-4b30-95db-044f7874a22d",
   "metadata": {},
   "source": [
    "**Define PytorchLightning Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06b4d3bf-e21e-4220-afe4-518cdf97ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from datasets import load_metric\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da07a82d-6931-45c9-b087-a0d8c35a436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureBasedSequneceClassification(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Inspired by BERT paper's feature-based approach.\n",
    "    The API is built on top of AutoModel and AutoConfig, provided by HuggingFace.\n",
    "    \n",
    "    see: https://arxiv.org/pdf/1810.04805.pdf\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_args, \n",
    "        training_args,\n",
    "        id2label: Dict,\n",
    "        task_name: str,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # init model\n",
    "        self.config = AutoConfig.from_pretrained(\n",
    "            self.hparams.model_args.model_name_or_path, \n",
    "            num_labels=len(self.hparams.id2label),\n",
    "            id2label=self.hparams.id2label,\n",
    "            label2id={l:i for i, l in self.hparams.id2label.items()},\n",
    "            output_hidden_states=True # get all hidden states\n",
    "        )\n",
    "        self.plm = AutoModel.from_pretrained(\n",
    "            self.hparams.model_args.model_name_or_path, \n",
    "            config=self.config,\n",
    "            add_pooling_layer=False # drop the pooling layer\n",
    "        )\n",
    "        \n",
    "        self.num_labels = self.config.num_labels\n",
    "        for param in self.plm.parameters(): # freeze all pretrained layers.\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.half_num_encoder = len(self.plm.encoder.layer) // 2\n",
    "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(self.half_num_encoder * self.config.hidden_size, self.num_labels) # concat the last 1/2 layers\n",
    "        self.plm.init_weights()\n",
    "\n",
    "        # init metric\n",
    "        self.metric = load_metric('f1', self.hparams.task_name, experiment_id=datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\"))\n",
    "\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        outputs = self.plm(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        concatenated_hidden_states = torch.cat(outputs.hidden_states[-self.half_num_encoder:], -1)\n",
    "        first_token_tensor = concatenated_hidden_states[:,0]\n",
    "        logits = self.classifier(first_token_tensor)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:            \n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "    \n",
    "    def _step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        preds = logits.argmax(dim=-1)\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"y_true\": labels,\n",
    "            \"y_pred\": preds\n",
    "        }\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._step(batch, batch_idx)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._step(batch, batch_idx)\n",
    "\n",
    "#     def training_step_end(self, batch_parts):\n",
    "#         losses = torch.stack(batch_parts['loss']).mean()\n",
    "#         self.log('tr_loss', losses, on_step=True, prog_bar=True)\n",
    "#         return losses \n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        self.log('tr_avg_loss', loss, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        y_true = torch.cat([x['y_true'] for x in outputs]).detach().cpu().numpy()\n",
    "        y_pred = torch.cat([x['y_pred'] for x in outputs]).detach().cpu().numpy()\n",
    "        loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        \n",
    "        self.log('val_avg_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log_dict(self.metric.compute(predictions=y_pred, references=y_true, average='macro'), on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def setup(self, stage=None) -> None:\n",
    "        if stage == 'fit':\n",
    "            # Get dataloader by calling it - train_dataloader() is called after setup() by default\n",
    "            train_loader = self.train_dataloader()\n",
    "\n",
    "            # Calculate total steps\n",
    "            tb_size = self.hparams.training_args.train_batch_size * max(1, self.trainer.gpus)\n",
    "            ab_size = self.trainer.accumulate_grad_batches * float(self.trainer.max_epochs)\n",
    "            self.total_steps = (len(train_loader.dataset) // tb_size) // ab_size\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.training_args.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters, lr=self.hparams.training_args.learning_rate, eps=self.hparams.training_args.adam_epsilon\n",
    "        )\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=self.hparams.training_args.warmup_steps, num_training_steps=self.total_steps\n",
    "        )\n",
    "        scheduler = {'scheduler': scheduler, 'interval': 'step', 'frequency': 1}\n",
    "        return [optimizer], [scheduler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f94063f-698c-4499-847e-753896dce13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning import seed_everything\n",
    "seed_everything(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20db0877-1214-4f24-a512-9c2bf89db638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-483d06c09187902b\n",
      "Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-483d06c09187902b/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95035fd73584db58197be738f90b965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=46.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca523582f1f4789a13875e8f5450aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-small were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "dm.setup(stage='fit')\n",
    "model = FeatureBasedSequneceClassification(model_args, training_args, dm.id2label, data_args.finetuning_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18f560dc-9a18-4b05-a224-ed7179ae9d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2befba6f-1f39-4b86-aa55-ecd4a6d130f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(max_epochs=3, gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e7c1c55-4d4a-401c-b86e-a01c458d074f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name       | Type         | Params\n",
      "--------------------------------------------\n",
      "0 | plm        | RobertaModel | 67.5 M\n",
      "1 | dropout    | Dropout      | 0     \n",
      "2 | classifier | Linear       | 16.1 K\n",
      "--------------------------------------------\n",
      "16.1 K    Trainable params\n",
      "67.5 M    Non-trainable params\n",
      "67.5 M    Total params\n",
      "270.060   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Global seed set to 42\n",
      "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3713774da249a283146483514ad704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
